# Домашнее задание к занятию "12.4 Развертывание кластера на собственных серверах, лекция 2"

---
## Задание 1: Подготовить инвентарь kubespray

Новые тестовые кластеры требуют типичных простых настроек. Нужно подготовить инвентарь и проверить его работу. Требования к инвентарю:
* подготовка работы кластера из 5 нод: 1 мастер и 4 рабочие ноды;
* в качестве CRI — containerd;
* запуск etcd производить на мастере.


## Решение 1: Подготовить инвентарь kubespray

Выполнял данное задание в соответствии с данным [туториалом](https://github.com/aak74/kubernetes-for-beginners/tree/master/15-install/30-kubespray)

 - Мой hosts.yaml

<details><summary>Подробнее</summary>

```
all:
  hosts:
    node1:
      ansible_host: 192.168.0.1
      ip: 192.168.0.1
      access_ip: 192.168.0.1
      ansible_ssh_user: vagrant
      ansible_ssh_pass: vagrant
    node2:
      ansible_host: 192.168.0.2
      ip: 192.168.0.2
      access_ip: 192.168.0.2
      ansible_ssh_user: vagrant
      ansible_ssh_pass: vagrant
    node3:
      ansible_host: 192.168.0.3
      ip: 192.168.0.3
      access_ip: 192.168.0.3
      ansible_ssh_user: vagrant
      ansible_ssh_pass: vagrant
    node4:
      ansible_host: 192.168.0.4
      ip: 192.168.0.4
      access_ip: 192.168.0.4
      ansible_ssh_user: vagrant
      ansible_ssh_pass: vagrant
    node5:
      ansible_host: 192.168.0.5
      ip: 192.168.0.5
      access_ip: 192.168.0.5
      ansible_ssh_user: vagrant
      ansible_ssh_pass: vagrant
  children:
    kube_control_plane:
      hosts:
        node1:
    kube_node:
      hosts:
        node2:
        node3:
        node4:
        node5:
    etcd:
      hosts:
        node1:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
```

</details>

 - Мой результат

<details><summary>Подробнее</summary>

```
vagrant@server1:~/kubespray$ kubectl get nodes
NAME    STATUS   ROLES           AGE   VERSION
node1   Ready    control-plane   18m   v1.24.4
node2   Ready    <none>          16m   v1.24.4
node3   Ready    <none>          16m   v1.24.4
node4   Ready    <none>          16m   v1.24.4
node5   Ready    <none>          16m   v1.24.4

vagrant@server1:~/kubespray$ kubectl get pods -A -o wide
NAMESPACE     NAME                              READY   STATUS    RESTARTS      AGE   IP               NODE    NOMINATED NODE   READINESS GATES
kube-system   calico-node-2z2kc                 1/1     Running   1 (16m ago)   21m   192.168.0.1     node1   <none>           <none>
kube-system   calico-node-4rlm8                 1/1     Running   0             21m   192.168.0.4     node4   <none>           <none>
kube-system   calico-node-r8n9k                 1/1     Running   0             21m   192.168.0.2     node2   <none>           <none>
kube-system   calico-node-s2pf5                 1/1     Running   0             21m   192.168.0.3     node3   <none>           <none>
kube-system   calico-node-wh9lb                 1/1     Running   2 (16m ago)   21m   192.168.0.5     node5   <none>           <none>
kube-system   coredns-74d6c5659f-7l2w6          1/1     Running   0             13m   10.233.71.1      node3   <none>           <none>
kube-system   coredns-74d6c5659f-lwhs6          1/1     Running   0             15m   10.233.102.129   node1   <none>           <none>
kube-system   dns-autoscaler-59b8867c86-22pmn   1/1     Running   0             15m   10.233.102.130   node1   <none>           <none>
kube-system   kube-apiserver-node1              1/1     Running   1             25m   192.168.0.1     node1   <none>           <none>
kube-system   kube-controller-manager-node1     1/1     Running   5 (16m ago)   25m   192.168.0.1     node1   <none>           <none>
kube-system   kube-proxy-4v4qn                  1/1     Running   0             23m   192.168.0.1     node1   <none>           <none>
kube-system   kube-proxy-b6bpz                  1/1     Running   0             23m   192.168.0.3     node3   <none>           <none>
kube-system   kube-proxy-ctntr                  1/1     Running   0             23m   192.168.0.5     node5   <none>           <none>
kube-system   kube-proxy-s2zrr                  1/1     Running   0             23m   192.168.0.2     node2   <none>           <none>
kube-system   kube-proxy-wkbl8                  1/1     Running   0             23m   192.168.0.4     node4   <none>           <none>
kube-system   kube-scheduler-node1              1/1     Running   5 (15m ago)   25m   192.168.0.1     node1   <none>           <none>
kube-system   nginx-proxy-node2                 1/1     Running   0             22m   192.168.0.2     node2   <none>           <none>
kube-system   nginx-proxy-node3                 1/1     Running   0             22m   192.168.0.3     node3   <none>           <none>
kube-system   nginx-proxy-node4                 1/1     Running   0             22m   192.168.0.4     node4   <none>           <none>
kube-system   nginx-proxy-node5                 1/1     Running   0             22m   192.168.0.5     node5   <none>           <none>
kube-system   nodelocaldns-7n6v6                1/1     Running   0             15m   192.168.0.5     node5   <none>           <none>
kube-system   nodelocaldns-9cjhw                1/1     Running   0             15m   192.168.0.2     node2   <none>           <none>
kube-system   nodelocaldns-b7plr                1/1     Running   0             15m   192.168.0.1     node1   <none>           <none>
kube-system   nodelocaldns-dx854                1/1     Running   0             15m   192.168.0.3     node3   <none>           <none>
kube-system   nodelocaldns-rv26d                1/1     Running   0             15m   192.168.0.4     node4   <none>           <none>

vagrant@node1:~$ sudo crictl ps
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
8de5522e802dd       5f5175f39b19e       About a minute ago   Running             calico-node               2                   f56fc7fa631d7       calico-node-2z2kc
77a66bce06535       1e7da779960fc       About a minute ago   Running             autoscaler                1                   d7be1a394e28a       dns-autoscaler-59b8867c86-22pmn
ccfec128406a0       a4ca41631cc7a       About a minute ago   Running             coredns                   1                   f759e9fea642b       coredns-74d6c5659f-lwhs6
7cb08f5f84683       7a53d1e08ef58       2 minutes ago        Running             kube-proxy                2                   c1b2f340f05b6       kube-proxy-4v4qn
030c15468e0d2       5bae806f8f123       2 minutes ago        Running             node-cache                2                   cb25a519f6751       nodelocaldns-b7plr
5dcbadf984bd2       03fa22539fc1c       3 minutes ago        Running             kube-scheduler            7                   3bff21b0e852d       kube-scheduler-node1
8700547f805d4       6cab9d1bed1be       3 minutes ago        Running             kube-apiserver            3                   27a891bddb988       kube-apiserver-node1
76ec1cd316ee5       1f99cb6da9a82       3 minutes ago        Running             kube-controller-manager   7                   b99c6ffa882ff       kube-controller-manager-node1
```

</details>
